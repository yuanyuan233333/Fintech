{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using Neural Networks to create Recommender Systems\n",
        "In a sense, it is an evolution of the Singular Value Decomposition (SVD) - see https://colab.research.google.com/drive/1goBESj_Z3nQBKEyQA3nq_PL4yWzW2PkP?usp=sharing. The idea is more or less the same: finding latent variables. But SVD is based on a linear framework, while NN are inherently **non-linear**.\n",
        "\n",
        "We will see two different approaches with NN:\n",
        "* a plain Multi-Layer Perceptron (MLP);\n",
        "* a Two Tower model.\n",
        "\n",
        "They are both Neural Network models, but they have some key differences in their architecture (and use cases, too).\n",
        "\n",
        "## Using a Multi-Layer Perceptron (MLP) to define a Recommender System\n",
        "This approach involves using a NN to learn the patterns and relationships between customers and products or services, and using this information to make recommendations. It can be particularly effective when dealing with complex datasets and non-linear patterns.\n",
        "\n",
        "Here we will use some simulated data...\n",
        "\n",
        "* we define the number of customers, products, and features;\n",
        "* we then generate synthetic data for the customer and product features using the truncated normal distribution, with a mean of 0 and a standard deviation of 1, truncated to the range [-1, 1];\n",
        "* we  generate synthetic data for the product ratings using randint.\n",
        "\n",
        "We then **flatten the customer and product features into the X: these are the features**; each row of X corresponds to a combination of a customer and a product (grouped together), and each column of X corresponds to a single feature.\n",
        "\n",
        "The y vector contains the corresponding **ratings = response variables**.\n",
        "Rating can be an explicit judgment (think about Amazon, or Netflix), but in our case it is **implicit judgment** - for example relative frequency of purchase of a product/service (or a a click on a call to action)."
      ],
      "metadata": {
        "id": "7iIi1sN_Sc4b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGfxgrfhP0W-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import truncnorm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Define the number of customers, products, and features\n",
        "num_customers = 1000\n",
        "num_products = 50\n",
        "num_customer_features = 5\n",
        "num_product_features = 10\n",
        "\n",
        "# Generate synthetic data for customer and product features\n",
        "customer_features = truncnorm.rvs(-1, 1, size=(num_customers, num_customer_features))\n",
        "product_features = truncnorm.rvs(-1, 1, size=(num_products, num_product_features))\n",
        "\n",
        "# Generate synthetic data for product ratings\n",
        "ratings = np.random.randint(low=1, high=6, size=(num_customers, num_products))\n",
        "\n",
        "# Flatten customer and product features and ratings into X and y matrices\n",
        "X = np.concatenate((np.tile(customer_features, (num_products, 1)), np.repeat(product_features, num_customers, axis=0)), axis=1)\n",
        "y = ratings.reshape(-1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.tile(customer_features, (num_products, 1)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZC7xQxraqlY",
        "outputId": "98ad84ad-50fe-4431-eb52-6331af821fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.repeat(product_features, num_customers, axis=0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFs_plHla1H7",
        "outputId": "abd9fc7e-8de2-45c5-cb9b-4e9184a1255a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX26bJRR_lEY",
        "outputId": "9563cef3-580d-491a-b428-2e1ed57e024c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is4UKszu6zsT",
        "outputId": "9e20c4d4-d865-4d6b-950d-33c941cdff83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmwhv4t2_eY4",
        "outputId": "dcc8a140-8c98-4b2d-b8b4-afea19458904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd1taj3AAD2n",
        "outputId": "cd6a1d57-0f0d-4524-e06b-9ddc8f124e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000,)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and train the model\n",
        "Let's **define the neural network architecture**.\n",
        "* It's a Multi-Layer Perceptron (MLP);\n",
        "* It has just three layers (they can be more - but be aware of overfitting);\n",
        "* The input layer has the same number of neurons as the number of features in the X matrix;\n",
        "* There's a hidden layer with 64 neurons and a ReLU activation function;\n",
        "* There's another hidden layer with 32 neurons and a ReLU activation function;\n",
        "* Finally, we add an output layer with a single neuron and a linear activation function.\n",
        "\n",
        "We then compile the model using the **mean squared error loss function** because the response variable is a rating = a continuous variable. Note that we would insted use a **cross-entropy function in case of binary classification**, i.e., output ={0, 1}).\n",
        "We use the **Adam optimizer**.\n",
        "\n",
        "We **train the model** on the training set using 10 epochs and a batch size of 32.\n",
        "\n",
        "Then we **evaluate the model** on the test set and print out the mean squared error."
      ],
      "metadata": {
        "id": "0kPqrNmmTimq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=X.shape[1]))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Mean Squared Error: ', mse)\n"
      ],
      "metadata": {
        "id": "npp-5EjQTUlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba102ff9-c6af-4df4-d615-87db9a00a6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 5s 2ms/step - loss: 2.3085\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0229\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0114\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 2.0029\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0039\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 2.0002\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 1.9988\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 1.9959\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 3s 2ms/step - loss: 1.9939\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 3s 2ms/step - loss: 1.9910\n",
            "Mean Squared Error:  2.0460219383239746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction\n",
        "Now let's **generate new customer data** using the same truncated normal distribution as before, tile the data to match the number of products, concatenate the new customer data with the product features, bla bla bla.\n",
        "\n",
        "Then we make a **prediction for the ratings of the new data** using the predict method of the model, and **get the indices of the top-rated products**.\n",
        "\n",
        "Note the use of the *argsort* method of NumPy. We also reverse the indices using the [::-1] slicing syntax to get the top-rated products in descending order.\n",
        "For the sake of simplicity we take just the top 3 products using the [:3] slicing syntax, and in the end we print the indices of the top-rated products..."
      ],
      "metadata": {
        "id": "GngpvzodU2QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new customer data\n",
        "new_customer_features = truncnorm.rvs(-1, 1, size=(1, num_customer_features))\n",
        "\n",
        "# Tile the customer data to match the number of products\n",
        "new_customer_data = np.tile(new_customer_features, (num_products, 1))\n",
        "\n",
        "# Concatenate the new customer data with the product features\n",
        "new_data = np.concatenate((new_customer_data, product_features), axis=1)\n",
        "\n",
        "# Make a prediction for the ratings of the new data (i.e., ratings = goals' intensity)\n",
        "new_ratings = model.predict(new_data)\n",
        "\n",
        "# Get the indices of the top-rated products\n",
        "top_products = np.argsort(new_ratings, axis=0)[::-1][:3]\n",
        "\n",
        "# Print the top-rated products (i.e., goals/needs)\n",
        "print('Top Rated Products: ', top_products)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7pBAiHETVgI",
        "outputId": "467f0913-4a4f-4c5f-adc1-1d78cbd9e7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step\n",
            "Top Rated Products:  [[21]\n",
            " [11]\n",
            " [ 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a Two Tower Model\n",
        "The Two Tower model is a NN architecture specifically designed for recommendation systems.\n",
        "\n",
        "The model consists of **two separate sub-models, or \"towers\"**. Each tower is responsible for **encoding the features of either the user or the item being recommended**: the towers take the raw feature vectors representing the products and customers as input, and transform them into a lower-dimensional representation that should captures the most important information about the items and customers for making recommendations (more or less like PCA, but it's non-linear). This process of transforming the raw feature vectors into a lower-dimensional representation is referred to as **\"feature encoding\" or \"embedding.\"** The towers use neural networks to learn this encoding, which is a non-linear mapping function from the high-dimensional input space to a lower-dimensional feature space that preserves the most important information about the input.\n",
        "\n",
        "For those somewhat familiar with recommendation systems, the Two-Tower model combines the user-based and item-based approaches in a joint architecture:\n",
        "* the user tower in the Two-Tower model is similar to the user-based approach, as it takes in user features to produce a vector representation of the user;\n",
        "* the item (products) tower is similar to the item-based approach, as it takes in item features to produce a vector representation of the items;\n",
        "* the joint space where the two vector representations are combined can be thought of as a hybrid of the user and item space.\n",
        "\n",
        "**The two towers are trained jointly to learn how to encode products and customers**, and the **encoded features are concatenated and passed through one or more additional layers to make the final prediction** for the rating or preference of each customer for each item.\n",
        "This output is then used to make recommendations.\n",
        "\n",
        "### Let's generate some synthetic data\n",
        "We use a similar but different dataset, which emphasizes the difference between product-related features and customer-related features.\n",
        "Let's say we have:\n",
        "* N=1000 customers (i.e., examples, on the rows);\n",
        "* K1 = 5 features describing the customers;\n",
        "* K2 = 3 features describing Q = 5 products;\n",
        "* Q = 5 response variables for each customer, representing an implicit or explicit rating given by each customer to 5 different products."
      ],
      "metadata": {
        "id": "0Slsq_hOeSA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Generate synthetic data\n",
        "N = 1000  # number of customers\n",
        "K1 = 5  # number of features describing customers\n",
        "K2 = 3  # number of features describing products\n",
        "Q = 5  # number of products and response variables\n",
        "Q2 = 1  # number of products and response variables\n",
        "\n",
        "np.random.seed(123)  # set the random seed for reproducibility\n",
        "\n",
        "# Generate synthetic data for customers, products, and ratings\n",
        "X1 = np.random.rand(N, K1)  # customer features\n",
        "X2 = np.random.rand(Q, K2)  # product features\n",
        "ratings = np.random.rand(N, Q2)  # ratings given by each customer to each product"
      ],
      "metadata": {
        "id": "IMX3nAYOlmYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the **model architecture** - again MSE and Adam optimizer (...see above MLP) - then **train the model**.\n",
        "\n",
        "Note: Since it's a similar example to before, for the sake of simplicity let's skip data splitting - for that step see the previous example on MLP."
      ],
      "metadata": {
        "id": "8kl42FdTnhZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Two Tower model\n",
        "input1 = Input(shape=(K1,))  # input layer for customer features\n",
        "input2 = Input(shape=(K2,))  # input layer for product features\n",
        "x1 = Dense(32, activation='relu')(input1)  # first hidden layer for customer features\n",
        "x2 = Dense(32, activation='relu')(input2)  # first hidden layer for product features\n",
        "x = Concatenate()([x1, x2])  # concatenate the output from the two hidden layers\n",
        "output = Dense(Q2, activation='sigmoid')(x)  # output layer for predicted ratings\n",
        "model = Model(inputs=[input1, input2], outputs=output)  # define the model\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam')  # compile the model with mean squared error loss and Adam optimizer\n",
        "\n",
        "# Train the model on the synthetic dataset\n",
        "model.fit([X1, np.tile(X2, (N, 1))], ratings, epochs=50, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxxiWouUnD_O",
        "outputId": "7272508a-4b41-4b1c-ff34-41dea5c2ee4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "25/25 [==============================] - 1s 9ms/step - loss: 0.0800 - val_loss: 0.0947\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0789 - val_loss: 0.0931\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0789 - val_loss: 0.0930\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0788 - val_loss: 0.0932\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 0.0935\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0786 - val_loss: 0.0928\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0788 - val_loss: 0.0934\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0786 - val_loss: 0.0931\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0786 - val_loss: 0.0929\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0789 - val_loss: 0.0931\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 0.0931\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0930\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0933\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0931\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0786 - val_loss: 0.0928\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.0933\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 0.0934\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0784 - val_loss: 0.0928\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0785 - val_loss: 0.0930\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.0928\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0929\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.0929\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.0928\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.0928\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0789 - val_loss: 0.0927\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0927\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0782 - val_loss: 0.0929\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0781 - val_loss: 0.0927\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0782 - val_loss: 0.0931\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0785 - val_loss: 0.0927\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 0.0926\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.0932\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0788 - val_loss: 0.0927\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0784 - val_loss: 0.0932\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0780 - val_loss: 0.0927\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0781 - val_loss: 0.0929\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0781 - val_loss: 0.0927\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0781 - val_loss: 0.0931\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0780 - val_loss: 0.0930\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0780 - val_loss: 0.0927\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0779 - val_loss: 0.0931\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 0.0931\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0780 - val_loss: 0.0931\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0780 - val_loss: 0.0930\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0780 - val_loss: 0.0932\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0780 - val_loss: 0.0927\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.0779 - val_loss: 0.0933\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0778 - val_loss: 0.0929\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0780 - val_loss: 0.0932\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0778 - val_loss: 0.0930\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x781a8432e650>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is then used to **make predictions** on new synthetic data with the same characteristics as the training data, but with 10 new customers. The predicted ratings for each new customer and each product are outputted: the expected rating is the intensity of goal/need. **To identify the recommendation, just sort the rating forecasts from highest to lowest and find the associated goal based portfolios.**"
      ],
      "metadata": {
        "id": "jLnWB0hzpHNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new synthetic data for prediction\n",
        "N_new = 10  # number of new customers\n",
        "X1_new = np.random.rand(N_new, K1)  # new customer features\n",
        "X2_new = np.random.rand(Q, K2)  # product features for the existing products\n",
        "\n",
        "# Do prediction on new data\n",
        "predicted_ratings = model.predict([X1_new, np.tile(X2_new, (N_new, 1))[:N_new]])  # predict ratings for the new customers\n",
        "print(predicted_ratings)  # print the predicted ratings = top goals/needs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BVFS22RpBYJ",
        "outputId": "b6b9fc70-e61a-4639-ffc9-b11af7be609e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 192ms/step\n",
            "[[0.46298596]\n",
            " [0.5108748 ]\n",
            " [0.52795464]\n",
            " [0.53764   ]\n",
            " [0.56107664]\n",
            " [0.4699516 ]\n",
            " [0.51727223]\n",
            " [0.5187645 ]\n",
            " [0.551029  ]\n",
            " [0.50386727]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DZWy-7ni7Jg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}